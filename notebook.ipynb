{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be019f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rizal\\anaconda3\\envs\\fppso\\lib\\site-packages\\evidently\\core\\metric_types.py:375: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
      "  np_bool = np.bool  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import skops.io as sio\n",
    "from evidently.presets import DataDriftPreset\n",
    "from evidently import Report\n",
    "from evidently import Dataset, DataDefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461ecfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Setup direktori yang diperlukan\"\"\"\n",
    "    directories = [\"models\", \"results\", \"monitoring\", \"monitoring/evidently_reports\"]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    print(\"âœ… Directories setup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27d0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load dan prepare data dengan Evidently monitoring integration\"\"\"\n",
    "    print(\"ğŸ” Loading and preparing data...\")\n",
    "    \n",
    "    # Path data files yang konsisten dengan evidently_monitoring.py\n",
    "    data_files = [\n",
    "        \"data/mental_health_lite.csv\", \n",
    "        \"data/mental_health_life_cut.csv\"\n",
    "    ]\n",
    "    \n",
    "    df = None\n",
    "    data_source = None\n",
    "    \n",
    "    # Cari file data yang tersedia\n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"âœ… Found data file: {file_path}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                data_source = file_path\n",
    "                print(f\"âœ… Data loaded successfully from {file_path}\")\n",
    "                print(f\"ğŸ“Š Data shape: {df.shape}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error loading {file_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if df is None:\n",
    "        raise FileNotFoundError(\"âŒ No valid dataset found in data/ folder\")\n",
    "    \n",
    "    # Create data loading summary untuk monitoring\n",
    "    data_summary = {\n",
    "        \"data_loaded\": True,\n",
    "        \"source_file\": data_source,\n",
    "        \"shape\": df.shape,\n",
    "        \"columns\": list(df.columns),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"missing_values\": df.isnull().sum().to_dict(),\n",
    "        \"data_types\": df.dtypes.astype(str).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Save data summary\n",
    "    with open(\"monitoring/data_loading_summary.json\", \"w\") as f:\n",
    "        json.dump(data_summary, f, indent=2, default=str)\n",
    "    \n",
    "    return df, data_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd44811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evidently_data_report(df, data_source):\n",
    "    \"\"\"Create Evidently data quality report\"\"\"\n",
    "    print(\"ğŸ“Š Creating Evidently data quality report...\")\n",
    "    \n",
    "    try:\n",
    "        from evidently import Report, Dataset, DataDefinition\n",
    "        from evidently.presets import DataQualityPreset\n",
    "        import os\n",
    "        \n",
    "        # Buat direktori jika belum ada\n",
    "        os.makedirs(\"monitoring/evidently_reports\", exist_ok=True)\n",
    "        \n",
    "        # Identifikasi tipe kolom secara otomatis\n",
    "        numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Definisikan struktur data\n",
    "        data_definition = DataDefinition(\n",
    "            numerical_columns=numerical_columns,\n",
    "            categorical_columns=categorical_columns\n",
    "        )\n",
    "        \n",
    "        # Buat Dataset object\n",
    "        current_data = Dataset.from_pandas(\n",
    "            df,\n",
    "            data_definition=data_definition\n",
    "        )\n",
    "        \n",
    "        # Buat report dengan DataQualityPreset\n",
    "        data_report = Report([\n",
    "            DataQualityPreset()\n",
    "        ])\n",
    "        \n",
    "        # Jalankan report\n",
    "        data_report.run(current_data=current_data, reference_data=None)\n",
    "        \n",
    "        # Save HTML report\n",
    "        report_path = \"monitoring/evidently_reports/data_quality_report.html\"\n",
    "        data_report.save_html(report_path)\n",
    "        \n",
    "        # Extract key metrics dari report\n",
    "        report_dict = data_report.as_dict()\n",
    "        \n",
    "        # Hitung statistik dasar\n",
    "        missing_values_per_column = df.isnull().sum()\n",
    "        total_missing = missing_values_per_column.sum()\n",
    "        \n",
    "        data_quality_summary = {\n",
    "            \"report_generated\": True,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": data_source,\n",
    "            \"total_rows\": len(df),\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"numerical_columns\": len(numerical_columns),\n",
    "            \"categorical_columns\": len(categorical_columns),\n",
    "            \"missing_values_count\": int(total_missing),\n",
    "            \"missing_values_percentage\": round((total_missing / (len(df) * len(df.columns))) * 100, 2),\n",
    "            \"columns_with_missing\": missing_values_per_column[missing_values_per_column > 0].to_dict(),\n",
    "            \"report_path\": report_path,\n",
    "            \"data_definition\": {\n",
    "                \"numerical_columns\": numerical_columns,\n",
    "                \"categorical_columns\": categorical_columns\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        with open(\"monitoring/evidently_data_quality.json\", \"w\") as f:\n",
    "            json.dump(data_quality_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Evidently data quality report saved to {report_path}\")\n",
    "        print(f\"ğŸ“ˆ Data summary: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        print(f\"ğŸ”¢ Numerical columns: {len(numerical_columns)}\")\n",
    "        print(f\"ğŸ“ Categorical columns: {len(categorical_columns)}\")\n",
    "        print(f\"âŒ Missing values: {total_missing} ({data_quality_summary['missing_values_percentage']}%)\")\n",
    "        \n",
    "        return data_quality_summary\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Import error - Evidently version mismatch: {e}\")\n",
    "        print(\"ğŸ’¡ Tip: Pastikan menggunakan evidently>=0.7.0\")\n",
    "        \n",
    "        # Create fallback summary\n",
    "        fallback_summary = {\n",
    "            \"report_generated\": False,\n",
    "            \"error\": f\"Import error: {str(e)}\",\n",
    "            \"error_type\": \"import_error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": data_source,\n",
    "            \"total_rows\": len(df),\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"missing_values_count\": int(df.isnull().sum().sum())\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/evidently_data_quality.json\", \"w\") as f:\n",
    "            json.dump(fallback_summary, f, indent=2)\n",
    "            \n",
    "        return fallback_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Evidently data quality report failed: {e}\")\n",
    "        \n",
    "        # Create fallback summary\n",
    "        fallback_summary = {\n",
    "            \"report_generated\": False,\n",
    "            \"error\": str(e),\n",
    "            \"error_type\": \"runtime_error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": data_source,\n",
    "            \"total_rows\": len(df),\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"missing_values_count\": int(df.isnull().sum().sum())\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/evidently_data_quality.json\", \"w\") as f:\n",
    "            json.dump(fallback_summary, f, indent=2)\n",
    "            \n",
    "        return fallback_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4297a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical features dengan proper handling\"\"\"\n",
    "    print(\"ğŸ”§ Encoding categorical features...\")\n",
    "    \n",
    "    encoders = {}\n",
    "    \n",
    "    # Identifikasi kolom kategorik\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Exclude target column jika ada\n",
    "    target_candidates = ['mental_health_condition', 'target', 'label', 'class']\n",
    "    target_column = None\n",
    "    \n",
    "    for col in target_candidates:\n",
    "        if col in df.columns:\n",
    "            target_column = col\n",
    "            if col in categorical_columns:\n",
    "                categorical_columns.remove(col)\n",
    "            break\n",
    "    \n",
    "    print(f\"ğŸ¯ Target column identified: {target_column}\")\n",
    "    print(f\"ğŸ“ Categorical columns to encode: {categorical_columns}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            encoder = LabelEncoder()\n",
    "            # Handle missing values\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "            df[f\"{col}_encoded\"] = encoder.fit_transform(df[col])\n",
    "            encoders[col] = encoder\n",
    "            print(f\"âœ… Encoded {col} -> {col}_encoded\")\n",
    "    \n",
    "    # Encode target column jika kategorik\n",
    "    if target_column and df[target_column].dtype == 'object':\n",
    "        target_encoder = LabelEncoder()\n",
    "        df[f\"{target_column}_encoded\"] = target_encoder.fit_transform(df[target_column])\n",
    "        encoders['target'] = target_encoder\n",
    "        target_column = f\"{target_column}_encoded\"\n",
    "        print(f\"âœ… Encoded target column: {target_column}\")\n",
    "    \n",
    "    return df, encoders, target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add95cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, target_column):\n",
    "    \"\"\"Prepare features untuk training\"\"\"\n",
    "    print(\"ğŸ¯ Preparing features for training...\")\n",
    "    \n",
    "    # Exclude non-feature columns\n",
    "    exclude_columns = [\n",
    "        target_column,\n",
    "        'id', 'index', 'timestamp', 'date'\n",
    "    ]\n",
    "    \n",
    "    # Add original categorical columns to exclude\n",
    "    categorical_originals = [col for col in df.columns if col.endswith('_encoded')]\n",
    "    for encoded_col in categorical_originals:\n",
    "        original_col = encoded_col.replace('_encoded', '')\n",
    "        if original_col in df.columns:\n",
    "            exclude_columns.append(original_col)\n",
    "    \n",
    "    # Select feature columns\n",
    "    feature_columns = []\n",
    "    for col in df.columns:\n",
    "        if col not in exclude_columns and df[col].dtype in ['int64', 'float64']:\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    print(f\"ğŸ“Š Selected features: {feature_columns}\")\n",
    "    print(f\"ğŸ¯ Target column: {target_column}\")\n",
    "    \n",
    "    if len(feature_columns) == 0:\n",
    "        raise ValueError(\"âŒ No valid numeric features found for training\")\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba23cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, X_test, y_train, y_test, feature_columns):\n",
    "    \"\"\"Train multiple models dan return best model\"\"\"\n",
    "    print(\"ğŸ¤– Training multiple models...\")\n",
    "    \n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            verbose=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Create pipeline dengan StandardScaler\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"ğŸ”„ Training {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', model)\n",
    "            ])\n",
    "            \n",
    "            # Train model\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            y_pred_proba = pipeline.predict_proba(X_test) if hasattr(pipeline, 'predict_proba') else None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            \n",
    "            # Cross validation\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            \n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'f1_weighted': f1_weighted,\n",
    "                'f1_macro': f1_macro,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()\n",
    "            }\n",
    "            \n",
    "            trained_models[name] = pipeline\n",
    "            \n",
    "            print(f\"âœ… {name} - Accuracy: {accuracy:.4f}, F1: {f1_weighted:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error training {name}: {e}\")\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(\n",
    "        [name for name in results.keys() if 'error' not in results[name]], \n",
    "        key=lambda x: results[x]['accuracy']\n",
    "    )\n",
    "    \n",
    "    best_model = trained_models[best_model_name]\n",
    "    best_accuracy = results[best_model_name]['accuracy']\n",
    "    \n",
    "    print(f\"ğŸ† Best model: {best_model_name} (Accuracy: {best_accuracy:.4f})\")\n",
    "    \n",
    "    return best_model, best_model_name, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845dc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_metadata(model, model_name, results, feature_columns, encoders):\n",
    "    \"\"\"Save model dan metadata\"\"\"\n",
    "    print(\"ğŸ’¾ Saving model and metadata...\")\n",
    "    \n",
    "    # Save model dengan skops\n",
    "    model_path = f\"models/best_model_{model_name.lower()}.skops\"\n",
    "    sio.dump(model, model_path)\n",
    "    \n",
    "    # Save encoders\n",
    "    encoders_path = \"models/encoders.pkl\"\n",
    "    with open(encoders_path, 'wb') as f:\n",
    "        pickle.dump(encoders, f)\n",
    "    \n",
    "    # Create comprehensive metadata\n",
    "    metadata = {\n",
    "        'model_name': model_name,\n",
    "        'model_path': model_path,\n",
    "        'encoders_path': encoders_path,\n",
    "        'feature_columns': feature_columns,\n",
    "        'training_timestamp': datetime.now().isoformat(),\n",
    "        'model_performance': results[model_name],\n",
    "        'all_models_performance': results,\n",
    "        'best_accuracy': results[model_name]['accuracy'],\n",
    "        'model_type': 'classification',\n",
    "        'framework': 'sklearn_pipeline'\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = \"models/model_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"âœ… Model saved to {model_path}\")\n",
    "    print(f\"âœ… Metadata saved to {metadata_path}\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18587197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_summary(metadata, data_source):\n",
    "    \"\"\"Create comprehensive training summary untuk monitoring\"\"\"\n",
    "    print(\"ğŸ“‹ Creating training summary...\")\n",
    "    \n",
    "    training_summary = {\n",
    "        \"training_completed\": True,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"data_source\": data_source,\n",
    "        \"best_model\": metadata['model_name'],\n",
    "        \"best_accuracy\": metadata['best_accuracy'],\n",
    "        \"models_trained\": list(metadata['all_models_performance'].keys()),\n",
    "        \"feature_count\": len(metadata['feature_columns']),\n",
    "        \"status\": \"success\",\n",
    "        \"model_path\": metadata['model_path'],\n",
    "        \"metadata_path\": \"models/model_metadata.json\"\n",
    "    }\n",
    "    \n",
    "    # Save training summary\n",
    "    with open(\"monitoring/training_summary.json\", \"w\") as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "    \n",
    "    # Save untuk CML report\n",
    "    with open(\"results/training_results.json\", \"w\") as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "    \n",
    "    print(\"âœ… Training summary created for monitoring integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3ec23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting ML Training Pipeline with Evidently Integration\n",
      "============================================================\n",
      "âœ… Directories setup completed\n",
      "ğŸ” Loading and preparing data...\n",
      "âœ… Found data file: data/mental_health_lite.csv\n",
      "âœ… Data loaded successfully from data/mental_health_lite.csv\n",
      "ğŸ“Š Data shape: (999, 14)\n",
      "ğŸ“Š Creating Evidently data quality report...\n",
      "âš ï¸ Import error - Evidently version mismatch: cannot import name 'DataQualityPreset' from 'evidently.presets' (c:\\Users\\rizal\\anaconda3\\envs\\fppso\\lib\\site-packages\\evidently\\presets\\__init__.py)\n",
      "ğŸ’¡ Tip: Pastikan menggunakan evidently>=0.7.0\n",
      "ğŸ”§ Encoding categorical features...\n",
      "ğŸ¯ Target column identified: None\n",
      "ğŸ“ Categorical columns to encode: ['gender', 'employment_status', 'work_environment', 'mental_health_history', 'seeks_treatment', 'mental_health_risk']\n",
      "âœ… Encoded gender -> gender_encoded\n",
      "âœ… Encoded employment_status -> employment_status_encoded\n",
      "âœ… Encoded work_environment -> work_environment_encoded\n",
      "âœ… Encoded mental_health_history -> mental_health_history_encoded\n",
      "âœ… Encoded seeks_treatment -> seeks_treatment_encoded\n",
      "âœ… Encoded mental_health_risk -> mental_health_risk_encoded\n",
      "âš ï¸ Using last column as target (fallback)\n",
      "ğŸ¯ Fallback target: mental_health_risk_encoded\n",
      "âœ… Using target column: mental_health_risk_encoded\n",
      "ğŸ“Š Target distribution:\n",
      "mental_health_risk_encoded\n",
      "2    588\n",
      "0    231\n",
      "1    180\n",
      "Name: count, dtype: int64\n",
      "ğŸ¯ Preparing features for training...\n",
      "ğŸ“Š Selected features: ['age', 'stress_level', 'sleep_hours', 'physical_activity_days', 'depression_score', 'anxiety_score', 'social_support_score', 'productivity_score']\n",
      "ğŸ¯ Target column: mental_health_risk_encoded\n",
      "ğŸ“Š Training data shape: X=(999, 8), y=(999,)\n",
      "ğŸ“Š Train: (799, 8), Test: (200, 8)\n",
      "ğŸ¤– Training multiple models...\n",
      "ğŸ”„ Training RandomForest...\n",
      "âœ… RandomForest - Accuracy: 0.9150, F1: 0.9146\n",
      "ğŸ”„ Training XGBoost...\n",
      "âœ… XGBoost - Accuracy: 0.9600, F1: 0.9600\n",
      "ğŸ”„ Training LightGBM...\n",
      "âœ… LightGBM - Accuracy: 0.9600, F1: 0.9601\n",
      "ğŸ† Best model: XGBoost (Accuracy: 0.9600)\n",
      "ğŸ’¾ Saving model and metadata...\n",
      "âœ… Model saved to models/best_model_xgboost.skops\n",
      "âœ… Metadata saved to models/model_metadata.json\n",
      "ğŸ“‹ Creating training summary...\n",
      "âœ… Training summary created for monitoring integration\n",
      "============================================================\n",
      "ğŸ‰ Training pipeline completed successfully!\n",
      "ğŸ† Best model: XGBoost\n",
      "ğŸ“Š Best accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    print(\"ğŸš€ Starting ML Training Pipeline with Evidently Integration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Setup directories\n",
    "        setup_directories()\n",
    "        \n",
    "        # Load and prepare data\n",
    "        df, data_source = load_and_prepare_data()\n",
    "        \n",
    "        # Create Evidently data quality report\n",
    "        create_evidently_data_report(df, data_source)\n",
    "        \n",
    "        # Encode categorical features\n",
    "        df, encoders, target_column = encode_categorical_features(df)\n",
    "        \n",
    "        # Jika masih None, coba manual selection\n",
    "        if target_column is None:\n",
    "                print(\"âš ï¸ Using last column as target (fallback)\")\n",
    "                target_column = df.columns[-1]\n",
    "                print(f\"ğŸ¯ Fallback target: {target_column}\")\n",
    "        \n",
    "        if target_column is None:\n",
    "            raise ValueError(\"âŒ No suitable target column found in dataset\")\n",
    "        \n",
    "        # Pastikan target column ada di dataframe\n",
    "        if target_column not in df.columns:\n",
    "            raise ValueError(f\"âŒ Target column '{target_column}' not found in dataframe\")\n",
    "        \n",
    "        print(f\"âœ… Using target column: {target_column}\")\n",
    "        print(f\"ğŸ“Š Target distribution:\\n{df[target_column].value_counts()}\")\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_columns = prepare_features(df, target_column)\n",
    "        \n",
    "        # Prepare training data\n",
    "        X = df[feature_columns]\n",
    "        y = df[target_column]\n",
    "        \n",
    "        print(f\"ğŸ“Š Training data shape: X={X.shape}, y={y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ“Š Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        # Train models\n",
    "        best_model, best_model_name, results = train_models(\n",
    "            X_train, X_test, y_train, y_test, feature_columns\n",
    "        )\n",
    "        \n",
    "        # Save model and metadata\n",
    "        metadata = save_model_and_metadata(\n",
    "            best_model, best_model_name, results, feature_columns, encoders\n",
    "        )\n",
    "        \n",
    "        # Create training summary\n",
    "        create_training_summary(metadata, data_source)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ‰ Training pipeline completed successfully!\")\n",
    "        print(f\"ğŸ† Best model: {best_model_name}\")\n",
    "        print(f\"ğŸ“Š Best accuracy: {metadata['best_accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training pipeline failed: {e}\")\n",
    "        \n",
    "        # Create error summary\n",
    "        error_summary = {\n",
    "            \"training_completed\": False,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"failed\"\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/training_summary.json\", \"w\") as f:\n",
    "            json.dump(error_summary, f, indent=2)\n",
    "        \n",
    "        with open(\"results/training_results.json\", \"w\") as f:\n",
    "            json.dump(error_summary, f, indent=2)\n",
    "        \n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fppso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
