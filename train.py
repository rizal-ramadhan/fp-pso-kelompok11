import pandas as pd
import numpy as np
import os
import json
import pickle
import joblib
from datetime import datetime
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.dummy import DummyClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Optional imports with fallbacks
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost not available")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸ LightGBM not available")

try:
    import skops.io as sio
    SKOPS_AVAILABLE = True
except ImportError:
    SKOPS_AVAILABLE = False
    print("âš ï¸ Skops not available, using joblib fallback")

try:
    from evidently.presets import DataDriftPreset, DataQualityPreset
    from evidently import Report, Dataset, DataDefinition
    EVIDENTLY_AVAILABLE = True
except ImportError:
    EVIDENTLY_AVAILABLE = False
    print("âš ï¸ Evidently not available")

def check_dependencies():
    """Check all required dependencies and provide status"""
    print("ğŸ” Checking dependencies...")
    
    dependencies_status = {}
    
    # Core libraries
    essential_libs = {
        'pandas': pd,
        'numpy': np,
        'sklearn': None
    }
    
    try:
        import sklearn
        essential_libs['sklearn'] = sklearn
        dependencies_status['sklearn'] = sklearn.__version__
    except ImportError:
        dependencies_status['sklearn'] = 'MISSING - CRITICAL'
    
    # Optional ML libraries
    optional_libs = {
        'xgboost': XGBOOST_AVAILABLE,
        'lightgbm': LIGHTGBM_AVAILABLE,
        'skops': SKOPS_AVAILABLE,
        'evidently': EVIDENTLY_AVAILABLE
    }
    
    dependencies_status.update({
        'pandas': pd.__version__,
        'numpy': np.__version__,
        'xgboost': 'Available' if XGBOOST_AVAILABLE else 'Missing',
        'lightgbm': 'Available' if LIGHTGBM_AVAILABLE else 'Missing',
        'skops': 'Available' if SKOPS_AVAILABLE else 'Missing',
        'evidently': 'Available' if EVIDENTLY_AVAILABLE else 'Missing',
        'joblib': 'Available',  # Always available with sklearn
        'matplotlib': 'Available',
        'seaborn': 'Available'
    })
    
    print("ğŸ“¦ Dependencies status:")
    for lib, status in dependencies_status.items():
        if 'MISSING - CRITICAL' in str(status):
            print(f"   âŒ {lib}: {status}")
        elif 'Missing' in str(status):
            print(f"   âš ï¸ {lib}: {status}")
        else:
            print(f"   âœ… {lib}: {status}")
    
    return dependencies_status

def setup_directories():
    """Setup required directories"""
    directories = [
        "model", 
        "results", 
        "monitoring", 
        "monitoring/evidently_reports",
        "explanations"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    print("âœ… Directories setup completed")

def load_and_prepare_data():
    """Load and prepare data with robust error handling"""
    print("ğŸ“Š Loading and preparing data...")
    
    # Try multiple data sources
    data_files = [
        'data/mental_health_lite.csv', 
        'data/mental_health_life_cut.csv',
        'mental_health_lite.csv',
        'mental_health_life_cut.csv'
    ]
    
    df = None
    data_source = None
    
    for file_path in data_files:
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path)
                data_source = file_path
                print(f"âœ… Dataset loaded from: {file_path}")
                break
            except Exception as e:
                print(f"âš ï¸ Failed to load {file_path}: {e}")
                continue
    
    if df is None:
        print("âŒ No dataset found, creating synthetic data for testing...")
        df = create_synthetic_data()
        data_source = "synthetic"
    
    print(f"ğŸ“Š Original data shape: {df.shape}")
    print(f"ğŸ“Š Original columns: {list(df.columns)}")
    
    # Clean and prepare data
    df_clean, encoders = clean_and_encode_data(df)
    
    return df_clean, encoders, data_source

def create_synthetic_data():
    """Create synthetic mental health data for testing"""
    print("ğŸ”¬ Creating synthetic data for testing...")
    
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        'age': np.random.randint(18, 65, n_samples),
        'stress_level': np.random.randint(1, 11, n_samples),
        'sleep_hours': np.random.normal(7, 1.5, n_samples),
        'exercise_hours': np.random.exponential(2, n_samples),
        'social_support': np.random.randint(1, 11, n_samples),
        'work_hours': np.random.normal(40, 10, n_samples),
        'income_level': np.random.randint(1, 6, n_samples),
        'gender': np.random.choice(['Male', 'Female', 'Other'], n_samples),
        'employment_status': np.random.choice(['Employed', 'Unemployed', 'Student', 'Retired'], n_samples),
        'work_environment': np.random.choice(['Office', 'Remote', 'Hybrid', 'Field'], n_samples),
        'mental_health_history': np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7]),
        'seeks_treatment': np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6])
    }
    
    # Create target variable based on features
    risk_scores = (
        (data['stress_level'] / 10 * 0.3) +
        ((10 - data['sleep_hours']) / 10 * 0.2) +
        ((10 - data['social_support']) / 10 * 0.2) +
        (data['age'] / 65 * 0.1) +
        np.random.normal(0, 0.1, n_samples)
    )
    
    data['mental_health_risk'] = np.where(risk_scores > 0.5, 'High', 
                                         np.where(risk_scores > 0.3, 'Medium', 'Low'))
    
    df = pd.DataFrame(data)
    print(f"âœ… Synthetic data created: {df.shape}")
    
    return df

def clean_and_encode_data(df):
    """Clean and encode data with proper handling"""
    print("ğŸ”§ Cleaning and encoding data...")
    
    df_clean = df.copy()
    encoders = {}
    
    # Identify categorical columns
    categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()
    
    # Identify target column
    target_candidates = ['mental_health_risk', 'mental_health_condition', 'target', 'risk']
    target_col = None
    
    for col in target_candidates:
        if col in df_clean.columns:
            target_col = col
            break
    
    if target_col is None:
        raise ValueError("No target column found. Expected one of: " + str(target_candidates))
    
    print(f"ğŸ¯ Target column identified: {target_col}")
    
    # Encode categorical features
    for col in categorical_cols:
        if col == target_col:
            continue
            
        print(f"ğŸ”§ Encoding {col}...")
        le = LabelEncoder()
        
        # Handle missing values
        df_clean[col] = df_clean[col].fillna('Unknown')
        
        # Encode to numeric
        df_clean[f'{col}_encoded'] = le.fit_transform(df_clean[col].astype(str))
        encoders[col] = le
        
        print(f"âœ… {col} encoded: {len(le.classes_)} unique values")
    
    # Encode target variable
    le_target = LabelEncoder()
    df_clean[target_col] = df_clean[target_col].fillna('Unknown')
    df_clean['risk_encoded'] = le_target.fit_transform(df_clean[target_col].astype(str))
    encoders['target'] = le_target
    
    print(f"âœ… Target encoded: {le_target.classes_}")
    
    # Remove original categorical columns
    columns_to_drop = [col for col in categorical_cols]
    df_clean = df_clean.drop(columns=columns_to_drop, errors='ignore')
    
    # Ensure all remaining columns are numeric
    for col in df_clean.columns:
        if col != 'risk_encoded':
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
    
    # Fill NaN values
    df_clean = df_clean.fillna(0)
    
    # Convert to proper dtypes
    feature_columns = [col for col in df_clean.columns if col != 'risk_encoded']
    for col in feature_columns:
        df_clean[col] = df_clean[col].astype(np.float32)
    
    df_clean['risk_encoded'] = df_clean['risk_encoded'].astype(np.int32)
    
    print(f"ğŸ“Š Cleaned data shape: {df_clean.shape}")
    print(f"ğŸ“Š Feature columns: {len(feature_columns)}")
    print(f"ğŸ“Š Target distribution:\n{df_clean['risk_encoded'].value_counts()}")
    
    return df_clean, encoders

def create_evidently_report(df, data_source):
    """Create Evidently data quality report with fallback"""
    print("ğŸ“Š Creating data quality report...")
    
    report_summary = {
        "report_generated": False,
        "timestamp": datetime.now().isoformat(),
        "data_source": data_source,
        "total_rows": len(df),
        "total_columns": len(df.columns),
        "missing_values_count": int(df.isnull().sum().sum())
    }
    
    if not EVIDENTLY_AVAILABLE:
        print("âš ï¸ Evidently not available, creating basic report...")
        
        # Basic data quality metrics
        report_summary.update({
            "evidently_available": False,
            "basic_stats": {
                "missing_percentage": round((df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100, 2),
                "numerical_columns": len(df.select_dtypes(include=['number']).columns),
                "categorical_columns": len(df.select_dtypes(include=['object']).columns)
            }
        })
    else:
        try:
            # Create Evidently report
            numerical_columns = df.select_dtypes(include=['number']).columns.tolist()
            categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
            
            data_definition = DataDefinition(
                numerical_columns=numerical_columns,
                categorical_columns=categorical_columns
            )
            
            current_data = Dataset.from_pandas(df, data_definition=data_definition)
            
            data_report = Report([DataQualityPreset()])
            data_report.run(current_data=current_data, reference_data=None)
            
            # Save HTML report
            report_path = "monitoring/evidently_reports/data_quality_report.html"
            data_report.save_html(report_path)
            
            report_summary.update({
                "report_generated": True,
                "evidently_available": True,
                "report_path": report_path,
                "numerical_columns": len(numerical_columns),
                "categorical_columns": len(categorical_columns)
            })
            
            print(f"âœ… Evidently report saved to {report_path}")
            
        except Exception as e:
            print(f"âš ï¸ Evidently report failed: {e}")
            report_summary["evidently_error"] = str(e)
    
    # Save report summary
    with open("monitoring/data_quality_report.json", "w") as f:
        json.dump(report_summary, f, indent=2)
    
    return report_summary

def train_models_robust(X_train, X_test, y_train, y_test, feature_columns):
    """Train multiple models with robust error handling"""
    print("ğŸ¤– Training models with robust error handling...")
    
    # Ensure data types
    X_train = X_train.astype(np.float32)
    X_test = X_test.astype(np.float32)
    y_train = y_train.astype(np.int32)
    y_test = y_test.astype(np.int32)
    
    # Define models
    models = {}
    
    # Always available: Random Forest
    models['RandomForest'] = RandomForestClassifier(
        n_estimators=50,
        random_state=42,
        max_depth=8,
        min_samples_split=5,
        n_jobs=1
    )
    
    # Optional: XGBoost
    if XGBOOST_AVAILABLE:
        models['XGBoost'] = xgb.XGBClassifier(
            n_estimators=50,
            random_state=42,
            max_depth=4,
            learning_rate=0.1,
            eval_metric='logloss',
            n_jobs=1
        )
    
    # Optional: LightGBM
    if LIGHTGBM_AVAILABLE:
        models['LightGBM'] = lgb.LGBMClassifier(
            n_estimators=50,
            random_state=42,
            max_depth=4,
            learning_rate=0.1,
            verbose=-1,
            force_col_wise=True,
            n_jobs=1
        )
    
    # Fallback: Dummy Classifier
    models['DummyClassifier'] = DummyClassifier(
        strategy='most_frequent',
        random_state=42
    )
    
    results = {}
    trained_models = {}
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    
    for name, model in models.items():
        print(f"Training {name}...")
        try:
            # Create pipeline with scaling
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', model)
            ])
            
            # Manual cross-validation
            cv_scores = []
            for train_idx, val_idx in cv.split(X_train, y_train):
                X_train_fold = X_train.iloc[train_idx]
                X_val_fold = X_train.iloc[val_idx]
                y_train_fold = y_train.iloc[train_idx]
                y_val_fold = y_train.iloc[val_idx]
                
                # Clone and train
                from sklearn.base import clone
                model_clone = clone(pipeline)
                model_clone.fit(X_train_fold, y_train_fold)
                
                y_pred_fold = model_clone.predict(X_val_fold)
                score = accuracy_score(y_val_fold, y_pred_fold)
                cv_scores.append(score)
            
            cv_scores = np.array(cv_scores)
            
            # Train on full training set
            pipeline.fit(X_train, y_train)
            
            # Test predictions
            y_pred = pipeline.predict(X_test)
            
            # Calculate metrics
            test_accuracy = accuracy_score(y_test, y_pred)
            f1_weighted = f1_score(y_test, y_pred, average='weighted')
            f1_macro = f1_score(y_test, y_pred, average='macro')
            
            # Store results
            results[name] = {
                'accuracy': float(test_accuracy),
                'f1_weighted': float(f1_weighted),
                'f1_macro': float(f1_macro),
                'cv_mean': float(cv_scores.mean()),
                'cv_std': float(cv_scores.std()),
                'cv_scores': cv_scores.tolist(),
                'training_successful': True
            }
            
            trained_models[name] = pipeline
            
            print(f"âœ… {name} - CV: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f}), Test: {test_accuracy:.4f}")
            
        except Exception as e:
            print(f"âŒ {name} training failed: {e}")
            results[name] = {
                'accuracy': 0.0,
                'f1_weighted': 0.0,
                'f1_macro': 0.0,
                'cv_mean': 0.0,
                'cv_std': 0.0,
                'error': str(e),
                'training_successful': False
            }
    
    # Select best model from successful ones
    successful_results = {name: result for name, result in results.items() 
                         if result.get('training_successful', False)}
    
    if not successful_results:
        raise ValueError("All models failed to train successfully")
    
    best_model_name = max(successful_results.keys(), key=lambda x: successful_results[x]['cv_mean'])
    best_model = trained_models[best_model_name]
    y_pred_best = best_model.predict(X_test)
    
    print(f"ğŸ† Best model: {best_model_name} (CV Score: {successful_results[best_model_name]['cv_mean']:.4f})")
    
    return best_model, best_model_name, results, y_pred_best

def save_model_robust(model, model_name, results, feature_columns, encoders):
    """Save model using multiple methods with fallbacks"""
    print("ğŸ’¾ Saving model with multiple fallback methods...")
    
    model_saved = False
    model_paths = []
    save_methods_tried = []
    
    # Method 1: Try skops if available
    if SKOPS_AVAILABLE:
        try:
            model_path = "model/mental_health_pipeline.skops"
            sio.dump(model, model_path)
            model_paths.append(model_path)
            model_saved = True
            save_methods_tried.append("skops")
            print(f"âœ… Model saved with skops: {model_path}")
        except Exception as e:
            print(f"âš ï¸ Skops save failed: {e}")
            save_methods_tried.append(f"skops_failed: {str(e)}")
    
    # Method 2: Try joblib (always available)
    if not model_saved:
        try:
            model_path = "model/mental_health_pipeline.joblib"
            joblib.dump(model, model_path)
            model_paths.append(model_path)
            model_saved = True
            save_methods_tried.append("joblib")
            print(f"âœ… Model saved with joblib: {model_path}")
        except Exception as e:
            print(f"âš ï¸ Joblib save failed: {e}")
            save_methods_tried.append(f"joblib_failed: {str(e)}")
    
    # Method 3: Try pickle as last resort
    if not model_saved:
        try:
            model_path = "model/mental_health_pipeline.pkl"
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            model_paths.append(model_path)
            model_saved = True
            save_methods_tried.append("pickle")
            print(f"âœ… Model saved with pickle: {model_path}")
        except Exception as e:
            print(f"âŒ All model save methods failed: {e}")
            save_methods_tried.append(f"pickle_failed: {str(e)}")
    
    # Save encoders and feature columns
    try:
        with open("model/encoders.pkl", "wb") as f:
            pickle.dump(encoders, f)
        
        with open("model/feature_columns.pkl", "wb") as f:
            pickle.dump(feature_columns, f)
            
        print("âœ… Encoders and feature columns saved")
    except Exception as e:
        print(f"âš ï¸ Error saving encoders/features: {e}")
    
    # Create comprehensive metadata
    metadata = {
        'model_saved': model_saved,
        'model_paths': model_paths,
        'save_methods_tried': save_methods_tried,
        'best_model_name': model_name,
        'feature_columns': feature_columns,
        'n_features': len(feature_columns),
        'training_timestamp': datetime.now().isoformat(),
        'training_status': 'completed' if model_saved else 'partial',
        'model_performance': {
            'test_accuracy': float(results[model_name]['accuracy']),
            'test_f1_weighted': float(results[model_name]['f1_weighted']),
            'test_f1_macro': float(results[model_name]['f1_macro']),
            'cv_mean': float(results[model_name]['cv_mean']),
            'cv_std': float(results[model_name]['cv_std'])
        },
        'all_models_performance': {name: {k: v for k, v in result.items() 
                                         if k not in ['cv_scores']} 
                                 for name, result in results.items()}
    }
    
    # Save metadata
    try:
        metadata_path = "model/model_metadata.json"
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2, default=str)
        print(f"âœ… Metadata saved: {metadata_path}")
    except Exception as e:
        print(f"âš ï¸ Error saving metadata: {e}")
    
    return metadata

def create_visualizations(model, model_name, results, feature_columns, y_test, y_pred):
    """Create and save visualizations"""
    print("ğŸ¨ Creating visualizations...")
    
    visualization_paths = {}
    
    try:
        # 1. Model Comparison Plot
        plt.figure(figsize=(12, 6))
        
        # Filter successful models
        successful_models = {name: result for name, result in results.items() 
                           if result.get('training_successful', False)}
        
        models = list(successful_models.keys())
        accuracies = [successful_models[model]['accuracy'] for model in models]
        cv_means = [successful_models[model]['cv_mean'] for model in models]
        cv_stds = [successful_models[model]['cv_std'] for model in models]
        
        x = np.arange(len(models))
        width = 0.35
        
        plt.bar(x - width/2, accuracies, width, label='Test Accuracy', alpha=0.8)
        plt.bar(x + width/2, cv_means, width, label='CV Mean', alpha=0.8)
        
        plt.xlabel('Models')
        plt.ylabel('Accuracy')
        plt.title('Model Performance Comparison')
        plt.xticks(x, models, rotation=45)
        plt.legend()
        plt.tight_layout()
        
        comparison_path = "results/model_comparison.png"
        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
        plt.close()
        visualization_paths['model_comparison'] = comparison_path
        print(f"âœ… Model comparison plot saved: {comparison_path}")
        
    except Exception as e:
        print(f"âš ï¸ Error creating model comparison plot: {e}")
    
    try:
        # 2. Confusion Matrix
        plt.figure(figsize=(8, 6))
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix - {model_name}')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        
        cm_path = "results/confusion_matrix.png"
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.close()
        visualization_paths['confusion_matrix'] = cm_path
        print(f"âœ… Confusion matrix saved: {cm_path}")
        
    except Exception as e:
        print(f"âš ï¸ Error creating confusion matrix: {e}")
    
    try:
        # 3. Feature Importance (if available)
        if hasattr(model.named_steps['classifier'], 'feature_importances_'):
            plt.figure(figsize=(12, 8))
            
            importances = model.named_steps['classifier'].feature_importances_
            indices = np.argsort(importances)[::-1]
            
            plt.bar(range(len(importances)), importances[indices])
            plt.title(f'Feature Importance - {model_name}')
            plt.xlabel('Features')
            plt.ylabel('Importance')
            
            feature_names = [feature_columns[i] for i in indices]
            plt.xticks(range(len(importances)), feature_names, rotation=45, ha='right')
            plt.tight_layout()
            
            importance_path = "results/feature_importance.png"
            plt.savefig(importance_path, dpi=300, bbox_inches='tight')
            plt.close()
            visualization_paths['feature_importance'] = importance_path
            print(f"âœ… Feature importance plot saved: {importance_path}")
            
    except Exception as e:
        print(f"âš ï¸ Error creating feature importance plot: {e}")
    
    return visualization_paths

def save_training_results(metadata, visualization_paths, data_source):
    """Save comprehensive training results"""
    print("ğŸ“‹ Saving training results...")
    
    # Create comprehensive training summary
    training_summary = {
        "training_completed": True,
        "timestamp": datetime.now().isoformat(),
        "data_source": data_source,
        "model_info": {
            "best_model": metadata['best_model_name'],
            "model_saved": metadata['model_saved'],
            "model_paths": metadata['model_paths'],
            "save_methods_tried": metadata['save_methods_tried']
        },
        "performance": metadata['model_performance'],
        "all_models": metadata['all_models_performance'],
        "data_info": {
            "n_features": metadata['n_features'],
            "feature_columns": metadata['feature_columns']
        },
        "visualization_paths": visualization_paths,
        "dependencies": {
            "xgboost_available": XGBOOST_AVAILABLE,
            "lightgbm_available": LIGHTGBM_AVAILABLE,
            "skops_available": SKOPS_AVAILABLE,
            "evidently_available": EVIDENTLY_AVAILABLE
        },
        "status": "success"
    }
    
    # Save in multiple locations for quality gates
    output_files = [
        "results/training_results.json",
        "monitoring/training_summary.json",
        "model/training_summary.json"
    ]
    
    for file_path in output_files:
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, "w") as f:
                json.dump(training_summary, f, indent=2)
            print(f"âœ… Training results saved: {file_path}")
        except Exception as e:
            print(f"âš ï¸ Error saving {file_path}: {e}")
    
    # Save metrics in text format
    try:
        metrics_path = "results/metrics.txt"
        with open(metrics_path, "w") as f:
            f.write(f"Training Results Summary\n")
            f.write(f"========================\n\n")
            f.write(f"Best Model: {metadata['best_model_name']}\n")
            f.write(f"Test Accuracy: {metadata['model_performance']['test_accuracy']:.4f}\n")
            f.write(f"Test F1 (Weighted): {metadata['model_performance']['test_f1_weighted']:.4f}\n")
            f.write(f"CV Mean: {metadata['model_performance']['cv_mean']:.4f}\n")
            f.write(f"CV Std: {metadata['model_performance']['cv_std']:.4f}\n\n")
            
            f.write("All Models Performance:\n")
            f.write("-" * 30 + "\n")
            for model_name, perf in metadata['all_models_performance'].items():
                if perf.get('training_successful', False):
                    f.write(f"{model_name}: {perf['cv_mean']:.4f} (+/- {perf['cv_std']:.4f})\n")
                else:
                    f.write(f"{model_name}: FAILED\n")
        
        print(f"âœ… Metrics saved: {metrics_path}")
    except Exception as e:
        print(f"âš ï¸ Error saving metrics: {e}")
    
    return training_summary

def ensure_quality_gate_outputs():
    """Ensure all required outputs exist for quality gates"""
    print("ğŸ” Ensuring quality gate outputs...")
    
    # Check for model files
    model_files = [
        "model/mental_health_pipeline.skops",
        "model/mental_health_pipeline.joblib", 
        "model/mental_health_pipeline.pkl"
    ]
    
    model_available = any(os.path.exists(path) for path in model_files)
    
    # Check for result files
    result_files = [
        "results/training_results.json",
        "monitoring/training_summary.json"
    ]
    
    results_available = all(os.path.exists(path) for path in result_files)
    
    # Create minimal outputs if missing
    if not results_available:
        minimal_results = {
            "training_completed": True,
            "timestamp": datetime.now().isoformat(),
            "model_available": model_available,
            "status": "completed",
            "quality_gate_check": True
        }
        
        for file_path in result_files:
            try:
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                with open(file_path, "w") as f:
                    json.dump(minimal_results, f, indent=2)
                print(f"âœ… Quality gate output created: {file_path}")
            except Exception as e:
                print(f"âš ï¸ Error creating {file_path}: {e}")
    
    print(f"âœ… Quality gate check completed - Model: {model_available}, Results: {results_available}")

def main():
    """Main training pipeline with comprehensive error handling"""
    print("ğŸš€ Starting ML Training Pipeline (Robust Version)")
    print("=" * 60)
    
    # Initialize status tracking
    training_status = {
        "dependencies_checked": False,
        "directories_setup": False,
        "data_loaded": False,
        "models_trained": False,
        "model_saved": False,
        "visualizations_created": False,
        "results_saved": False,
        "pipeline_completed": False,
        "errors": [],
        "warnings": []
    }
    
    try:
        # 1. Check dependencies
        print("\nğŸ” Step 1: Checking dependencies...")
        dependency_status = check_dependencies()
        training_status["dependencies_checked"] = True
        
        # 2. Setup directories
        print("\nğŸ“ Step 2: Setting up directories...")
        setup_directories()
        training_status["directories_setup"] = True
        
        # 3. Load and prepare data
        print("\nğŸ“Š Step 3: Loading and preparing data...")
        try:
            df, encoders, data_source = load_and_prepare_data()
            training_status["data_loaded"] = True
            print(f"âœ… Data loading completed from: {data_source}")
        except Exception as e:
            error_msg = f"Data loading failed: {str(e)}"
            training_status["errors"].append(error_msg)
            print(f"âŒ {error_msg}")
            raise
        
        # 4. Create data quality report
        print("\nğŸ“‹ Step 4: Creating data quality report...")
        try:
            data_report = create_evidently_report(df, data_source)
            print("âœ… Data quality report completed")
        except Exception as e:
            warning_msg = f"Data quality report failed: {str(e)}"
            training_status["warnings"].append(warning_msg)
            print(f"âš ï¸ {warning_msg}")
        
        # 5. Prepare training data
        print("\nğŸ¯ Step 5: Preparing training data...")
        try:
            # Separate features and target
            feature_columns = [col for col in df.columns if col != 'risk_encoded']
            X = df[feature_columns].copy()
            y = df['risk_encoded'].copy()
            
            print(f"ğŸ“Š Features: {len(feature_columns)}")
            print(f"ğŸ“Š Samples: {len(X)}")
            print(f"ğŸ“Š Target distribution:\n{y.value_counts()}")
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y
            )
            
            print(f"ğŸ“Š Train samples: {len(X_train)}")
            print(f"ğŸ“Š Test samples: {len(X_test)}")
            
        except Exception as e:
            error_msg = f"Data preparation failed: {str(e)}"
            training_status["errors"].append(error_msg)
            print(f"âŒ {error_msg}")
            raise
        
        # 6. Train models
        print("\nğŸ¤– Step 6: Training models...")
        try:
            best_model, best_model_name, results, y_pred = train_models_robust(
                X_train, X_test, y_train, y_test, feature_columns
            )
            training_status["models_trained"] = True
            print(f"âœ… Model training completed - Best: {best_model_name}")
        except Exception as e:
            error_msg = f"Model training failed: {str(e)}"
            training_status["errors"].append(error_msg)
            print(f"âŒ {error_msg}")
            raise
        
        # 7. Save model
        print("\nğŸ’¾ Step 7: Saving model...")
        try:
            metadata = save_model_robust(
                best_model, best_model_name, results, feature_columns, encoders
            )
            training_status["model_saved"] = metadata.get("model_saved", False)
            print(f"âœ… Model saving completed - Saved: {training_status['model_saved']}")
        except Exception as e:
            error_msg = f"Model saving failed: {str(e)}"
            training_status["errors"].append(error_msg)
            print(f"âš ï¸ {error_msg}")
            # Continue even if model saving fails
            metadata = {
                "model_saved": False,
                "best_model_name": best_model_name,
                "model_performance": {
                    "test_accuracy": float(results[best_model_name]['accuracy']),
                    "test_f1_weighted": float(results[best_model_name]['f1_weighted']),
                    "cv_mean": float(results[best_model_name]['cv_mean']),
                    "cv_std": float(results[best_model_name]['cv_std'])
                },
                "all_models_performance": results,
                "feature_columns": feature_columns,
                "n_features": len(feature_columns)
            }
        
        # 8. Create visualizations
        print("\nğŸ¨ Step 8: Creating visualizations...")
        try:
            visualization_paths = create_visualizations(
                best_model, best_model_name, results, feature_columns, y_test, y_pred
            )
            training_status["visualizations_created"] = len(visualization_paths) > 0
            print(f"âœ… Visualizations completed - Created: {len(visualization_paths)}")
        except Exception as e:
            warning_msg = f"Visualization creation failed: {str(e)}"
            training_status["warnings"].append(warning_msg)
            print(f"âš ï¸ {warning_msg}")
            visualization_paths = {}
        
        # 9. Save training results
        print("\nğŸ“‹ Step 9: Saving training results...")
        try:
            training_summary = save_training_results(metadata, visualization_paths, data_source)
            training_status["results_saved"] = True
            print("âœ… Training results saved")
        except Exception as e:
            error_msg = f"Results saving failed: {str(e)}"
            training_status["errors"].append(error_msg)
            print(f"âš ï¸ {error_msg}")
        
        # 10. Ensure quality gate outputs
        print("\nğŸ” Step 10: Ensuring quality gate outputs...")
        try:
            ensure_quality_gate_outputs()
            print("âœ… Quality gate outputs ensured")
        except Exception as e:
            warning_msg = f"Quality gate setup failed: {str(e)}"
            training_status["warnings"].append(warning_msg)
            print(f"âš ï¸ {warning_msg}")
        
        training_status["pipeline_completed"] = True
        
        # Final summary
        print("\n" + "=" * 60)
        print("ğŸ‰ TRAINING PIPELINE COMPLETED!")
        print("=" * 60)
        print(f"ğŸ† Best Model: {best_model_name}")
        print(f"ğŸ“Š Test Accuracy: {metadata['model_performance']['test_accuracy']:.4f}")
        print(f"ğŸ“Š CV Score: {metadata['model_performance']['cv_mean']:.4f} (+/- {metadata['model_performance']['cv_std']:.4f})")
        print(f"ğŸ’¾ Model Saved: {metadata.get('model_saved', False)}")
        print(f"ğŸ¨ Visualizations: {len(visualization_paths)}")
        
        if training_status["warnings"]:
            print(f"\nâš ï¸ Warnings ({len(training_status['warnings'])}):")
            for warning in training_status["warnings"]:
                print(f"   - {warning}")
        
        print(f"\nâœ… Pipeline Status: SUCCESS")
        
    except Exception as e:
        print(f"\nâŒ CRITICAL ERROR in training pipeline: {e}")
        print("=" * 60)
        
        # Save error status for quality gates
        error_summary = {
            "training_completed": False,
            "timestamp": datetime.now().isoformat(),
            "error": str(e),
            "training_status": training_status,
            "status": "failed",
            "dependencies": {
                "xgboost_available": XGBOOST_AVAILABLE,
                "lightgbm_available": LIGHTGBM_AVAILABLE,
                "skops_available": SKOPS_AVAILABLE,
                "evidently_available": EVIDENTLY_AVAILABLE
            }
        }
        
        # Ensure directories exist and save error summary
        error_output_files = [
            "results/training_results.json",
            "monitoring/training_summary.json",
            "model/training_summary.json"
        ]
        
        for file_path in error_output_files:
            try:
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                with open(file_path, "w") as f:
                    json.dump(error_summary, f, indent=2)
                print(f"ğŸ“‹ Error summary saved: {file_path}")
            except Exception as save_error:
                print(f"âš ï¸ Could not save error summary to {file_path}: {save_error}")
        
        # Still ensure quality gate outputs exist
        try:
            ensure_quality_gate_outputs()
        except Exception as qg_error:
            print(f"âš ï¸ Could not ensure quality gate outputs: {qg_error}")
        
        print(f"âŒ Pipeline Status: FAILED")
        raise
    
    finally:
        # Always create a final status report
        final_status = {
            "pipeline_execution_completed": True,
            "timestamp": datetime.now().isoformat(),
            "training_status": training_status,
            "final_state": "completed" if training_status.get("pipeline_completed", False) else "failed"
        }
        
        try:
            with open("monitoring/pipeline_status.json", "w") as f:
                json.dump(final_status, f, indent=2)
        except:
            pass  # Don't fail on final status save

if __name__ == "__main__":
    try:
        main()
        print("\nğŸ Script execution completed successfully!")
    except Exception as e:
        print(f"\nğŸ’¥ Script execution failed: {e}")
        import traceback
        traceback.print_exc()
        
        # Exit with error code for CI/CD
        import sys
        sys.exit(1)